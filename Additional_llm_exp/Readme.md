
## Additional Experiments: AVHubert vs AVHubert-LLM

This folder contains additional experiments comparing AVHubert with AVHubert-LLM based models. These are not part of the original ViPS metric, but are side experiments we conducted to explore whether the LLM-based approach learns anything different from the AVHubert pretrained model when used with ViPS.

## Citations


These experiments were performed using the VSP-LLM framework and AvHubert, as cited below:

> J. H. Yeo, S. Han, M. Kim, and Y. M. Ro, "Where Visual Speech Meets Language: VSP-LLM Framework for Efficient and Context-Aware Visual Speech Processing," arXiv preprint arXiv:2402.15151, 2024. [Online]. Available: https://arxiv.org/abs/2402.15151

> B. Shi, W.-N. Hsu, K. Lakhotia, and A. Mohamed, "Learning Audio-Visual Speech Representation by Masked Multimodal Cluster Prediction," arXiv preprint arXiv:2201.02184, 2022. [Online]. Available: https://arxiv.org/abs/2201.02184
